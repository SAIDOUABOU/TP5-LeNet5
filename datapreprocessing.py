#module datapreprocessing

"""
    Ce module contient deux classes:
      <>ImagesDataPreprocessingForMLP -> fournit des ensembles X_train, y_train, X_val, y_val, X_test, y_test pr√™ts √† utilis√©s dans un MLP.
      <>ImagesDataPreprocessingForCNN -> fournit des ensembles X_train, y_train, X_val, y_val, X_test, y_test pr√™ts √† utilis√©s dans un CNN.
      <>PreprocessingData             -> fournit des ensembles X_train, y_train, X_val, y_val, X_test, y_test √† partir des donn√©es tabulaires pr√™ts √† utilis√©s
                                         dans des MLP et des mod√®les de machine learning.
    NB: mode d'emploi de ces classe:
                                     ___________________________________________________________________________________________________________
                                     |PreprocessingData           | ImagesDataPreprocessingForCNN  |     ImagesDataPreprocessingForMLP         |
                                     |_________________________________________________________________________________________________________|
                                     |instanciation d'un objet    |     m√©thodes de classe         |     m√©thodes de classe                    |
                                     |Objet.fit_transform(...)    | Class.fit_transform(...)       |     Class.fit_transform                   |
                                     |____________________________|________________________________|___________________________________________|
"""

#importations
from tqdm import tqdm #sert √† afficher les barre de progression d'ex√©cution.

import pandas as pd
import numpy as np
from scipy.signal import correlate2d
from sklearn.preprocessing import LabelEncoder , OneHotEncoder
from sklearn.model_selection import train_test_split
import os
import cv2



################################################### classe imagesDataPreprocessing ##########################################################################

class ImagesDataPreprocessingForMLP:
    """
    la classe rassembles des m√©thodes de pr√©traitement des donn√©es images et le sur-√©chantillonage. ces m√©thodes sont des m√©thodes de la classe donc 
    elles vont √™tre appel√©es sans l'instanciation de la classe.
    la m√©thode fit_transforme() ex√©cute le pipeline de pr√©taitement et fournit comme sortie les ensembles de train, validation et de test avec les target
    cod√©s en One-hot pour √™tre compatible avec la fonction d'activation softmax et la fonction de perte 'cross-entropy'
    """
    def __init__(self):
        pass
    
    @classmethod
    def loadData_csvFile(self , path: str , file_namesAndLabelsOfImages: str) -> pd.DataFrame:
        #le chargement du fichier contenant les chemins vers les images et les labels en les mettant dans un 
        #DataFrame qui fera l'objet de return
        #validation de l'entr√©e
        assert isinstance(path , str) , "L'argument 1 pass√©e √† cette m√©thode doit √™tre un chemin vers un dossier 'string' !"
        assert isinstance(file_namesAndLabelsOfImages , str) , "L'arguement 2 doit √™tre un nom d'un fichier csv 'String' !"

        #d√©finition du chemin du dossier contenant les images et le fichier csv (noms des images + labels)
        pathOfDataDirectory = os.path.join(os.getcwd() , path)

        #chargement du fichier csv contenant √† la fois les noms des images et leurs √©tiquettes
        # Charger le fichier CSV contenant les tiquettes
        try:
            labelsAndPahs = pd.read_csv(os.path.join(pathOfDataDirectory , file_namesAndLabelsOfImages) , header= None)
            #ajouter les ent√™tes aux DataFrame
            labelsAndPahs.columns = ['image_path', 'label'] 
            
            """
             Supprimer le pr√©fixe "images-data-64/" du chemin si pr√©sent et cela concerne seulement notre tp3 car le fichier csv contient 
            un r√©pertoire qui ne figure dans le vrai chemin et en plus des slash √† la place des anti-slash
            """
            labelsAndPahs['image_path'] = labelsAndPahs['image_path'].apply(lambda p: p[len("./images-data-64/"):]
                                                                            if p.startswith("./images-data-64/") else p)

            #v√©rification de l'existance des noms des images et de leurs √©tiquettes dans le fichier csv
            assert (
                'image_path' in labelsAndPahs.columns and 'label' in labelsAndPahs.columns 
            ), "Le fichier doit contenir les noms des fichiers images et le √©tiquettes de chacune d'elles !"

            #ajout du r√©pertoire racine aux chemins de toutes les images
            labelsAndPahs['image_path'] = labelsAndPahs['image_path'].apply(lambda x: 
                                                                            os.path.normpath(os.path.join(pathOfDataDirectory, x.lstrip("/\\")))) 
            
            print(f"T√©l√©chargement de {len(labelsAndPahs)} √©chantillons avec {labelsAndPahs['label'].nunique()} classes")
            
            return labelsAndPahs
            
        except FileNotFoundError:
            print(f"Le fichier {file_namesAndLabelsOfImages} est introuvable !")
            return pd.DataFrame()

    @classmethod
    def loadDataFromDirectory(self , path: str) -> pd.DataFrame:
        # Consruction du DataFrame √† partir du r√©pertoire contenant des images
        #validation de l'entr√©e
        assert isinstance(path , str) , "L'argument doit √™tre le chemin vers le r√©pertoire des images sous forme de string !"
        imagePaths = []   #liste des chemins complets des images
        labels = []       #liste des √©tiquettes des images
        #d√©finition du chemin du dossier contenant les images et le fichier csv (noms des images + labels)
        pathOfDataDirectory = os.path.join(os.getcwd() , path)
        
        for subDirName in os.listdir(pathOfDataDirectory):
            subDirPath = os.path.join(pathOfDataDirectory , subDirName)
            if os.path.isdir(subDirPath):
                for imageName in os.listdir(subDirPath):
                    imagePaths.append(os.path.join(subDirPath , imageName))
                    labels.append(subDirPath)
        
        # Cr√©ation du DataFrame
        data_labels_imagesPaths = pd.DataFrame({'image_path': imagePaths , 'label': labels})
        
        # V√©rification du DataFrame avant de le renvoyer
        assert not data_labels_imagesPaths.empty, "Le data n'est pas t√©l√©charg√©, v√©rifier les chemins et le dossier source !"
        
        print(f"T√©l√©chargement de {len(data_labels_imagesPaths)} √©chantillons avec {data_labels_imagesPaths['label'].nunique()} classes")

        return data_labels_imagesPaths

    @classmethod
    def labelsEncoder(self , data: pd.DataFrame , labels: str ) -> pd.DataFrame:
        #Encodage  des labels des images
        #validation des entr√©es
        assert isinstance(data , pd.DataFrame) , "L'argument 1 doit √™tre une DataFrame !"
        assert isinstance(labels , str) and labels in data.columns , "L'argument 2 doit √™tre un nom d'une colonne de DataFrame 'arg1' !"
        
        encoder = LabelEncoder()
        data[labels] = encoder.fit_transform (data[labels])
        
        num_classes = len(encoder.classes_)
        #validation de l'op√©ration de l'encodage
        assert data[labels].nunique() == num_classes , "L'op√©ration de l'encodage n'est pas bien aboutie !"

        return data

    @classmethod
    def loadAndProcessImage(self , imagePath: str , sizeOfImage: tuple[int , int]) -> np.ndarray:
        #charge l'image et la redimensionner avant de la rendre sous forme d'une matrice
        #validatio des entr√©es
        assert isinstance(imagePath , str) , "L'argument 1 est un chemin vers une image sous forme de 'String' !"
        assert os.path.exists (imagePath) , f"L'image est introuvable: {imagePath } !"
        assert (
            isinstance(sizeOfImage , tuple) and np.all(isinstance(number , int)  and  number > 0 for number in sizeOfImage) 
        ) , "La taille de l'image doit √™tre un tuple de deux entiers strictement positifs !"

        #mettre l'image en gris
        image = cv2.imread(imagePath , cv2.IMREAD_GRAYSCALE)
        assert image is not None , f"L'image est introuvable: {imagePath } !"
        #redimensionner l'image
        image = cv2.resize (image , sizeOfImage )
        #normaliser l'image (x - min)/(max-min)
        image = image.astype (np.float32 )/255.0 

        #validation de la sortie
        assert isinstance(image , np.ndarray) and np.all((image >= 0) & (image <= 1)) , "l'image n'√©tait pas tra√Æt√©e correctement !"
        assert image.shape == sizeOfImage , "Le redimensionnement de l'image n'√©tait pas fait correctement !"
        
        return image.flatten()

    @classmethod
    def processAllImages(self , labelsAndPaths: pd.DataFrame , sizeOfImage: tuple[int , int]) -> np.ndarray:
        #transforme toutes les images en matrice de donn√©es et la target en vecteur
        #validation des entr√©es
        assert isinstance(labelsAndPaths , pd.DataFrame) , "L'argument 1 doit √™tre de type 'DataFrame' de pandas !"
        assert (
                'image_path' in labelsAndPaths.columns and 'label' in labelsAndPaths.columns 
                ), "Le Dataframe 'argument 1' doit contenir les chemins vers les fichiers images et le √©tiquettes de chacune d'elles !"
        assert (
                  isinstance(sizeOfImage , tuple) and np.all(isinstance(number , int)  and  number > 0 for number in sizeOfImage) 
                ) , "La taille de l'image doit √™tre un tuple de deux entiers strictement positifs !"
        
        #pr√©taitement des images
        X = np.array([self.loadAndProcessImage(path , sizeOfImage) for path in labelsAndPaths['image_path']])
        y = self.labelsEncoder(labelsAndPaths , 'label' )['label'].values

        # v√©rification des  dimensions
        assert X.shape[0] == y.shape[0] , "Dimension incoh√©rent entre les donn√©es X et target y !"
        assert (
                  X.shape[1] == sizeOfImage[0] * sizeOfImage[1]
        ), f"le nombre de features dans X {X.shape[1]} doit correspondre √† {sizeOfImage[0] * sizeOfImage[1]} !"

        return X , y


    @classmethod
    def splitData(self , X: np.ndarray , y: np.ndarray , test_size: float , stratification: bool= True , 
                  sameResultOfSplit: bool= True)  -> tuple[np.ndarray , ...]:
        #divise les donn√©es et le traget en sous ensembles de train, de validation et de test
        #validation des entr√©es
        assert isinstance(X , np.ndarray) , "l'argument1 doit √™tre de type ndarray de numpy !"
        assert isinstance(y , np.ndarray) , "l'argument 2 doit √™tre de type ndarray de numpy !"
        assert X.shape[0] == y.shape[0] , "des donn√©es et le target doivent avoir le m√™me nombre d'enregistrements !"
        assert isinstance(test_size , float) and  1 > test_size > 0 , "la taille de test doit √™tre un r√©el entre 0 et 1 !"
        assert isinstance(stratification , bool) , "L'argument 3 'stratification' doit prendre soit 'True' soit 'False' comme valeur!"
        assert isinstance(sameResultOfSplit , bool) , "L'argument 4 'sameResultOfSplit' doit prendre soit 'True' soit 'False' comme valeur!"
        

        #division en train et test selon les param√®tres 'booleen' pass√©s √† la m√©thode
        if stratification:
            if sameResultOfSplit:
                X_train , X_test , y_train , y_test = train_test_split(X , y , test_size= test_size  , stratify= y , random_state= 42)
            else:
                X_train , X_test , y_train , y_test = train_test_split(X , y , test_size= test_size , stratify= y)
        else:
            if sameResultOfSplit:
                X_train , X_test , y_train , y_test = train_test_split(X , y , test_size= test_size , random_state= 42)
            else:
                X_train , X_test , y_train , y_test = train_test_split(X , y , test_size= test_size)

        #validation des r√©sultat avant de les retourner
        assert X_train.shape[0] == y_train.shape[0] , "X_train et y_train doivent avoir le m√™me nombre d'√©chantillons!"
        assert X_train.shape[0] == y_train.shape[0] , "X_test et y_test doivent avoir le m√™me nombre d'√©chantillons!"
        assert X_train.shape[1] == X_test.shape[1] , "X_train et X_test doivent avoir le m√™me nombre de variables!"

        return X_train , X_test , y_train , y_test

    @classmethod
    def oneHotEncoder(self , y: np.ndarray) -> np.ndarray:
        #encode les labels en one-hot: sous forme de veteur contenant des z√©ros et un
        #validation de l'entr√©e
        assert isinstance(y , np.ndarray) and y.ndim == 1 , "L'argument doit √™tre de type ndarray d'une seule dimension !"

        y_one_hot = np.array(OneHotEncoder(sparse_output=False).fit_transform(y.reshape(-1, 1)))

        #validation de la sortie
        assert (
            isinstance(y_one_hot , np.ndarray) and y_one_hot.shape == (y.shape[0] , len(np.unique(y)))
        ) , "Le r√©sultat de oneHotEncoder n'est un ndarray ou ses dimensions sont incorrectes !"

        return y_one_hot

    @classmethod
    def fit_transform(self , path: str , file_namesAndLabelsOfImages: str= None , sizeOfImage: tuple[int , int]= (32 , 32) , val_size: float= 0.2 ,
                      test_size: float= 0.2 ,  stratification: bool= True , sameResultOfSplit: bool= True) -> tuple[np.ndarray, ...]:
        #cette m√©thode est un pipeline qui fait appel √† les autres m√©thode pour charger, pr√©traiter et diviser le data et retourne en fin 
        #retourner les ensembles de train, de validation et de test.
        #validation des l'entr√©es
        assert isinstance(path , str) , "L'argument 1 pass√©e √† cette m√©thode doit √™tre un chemin vers un dossier 'string' !"
        assert isinstance(file_namesAndLabelsOfImages , (type(None) , str)) , "L'arguement 2 doit √™tre un nom d'un fichier csv 'String' ou None !"
        assert (
            isinstance(sizeOfImage , tuple) and np.all(isinstance(number , int)  and  number > 0 for number in sizeOfImage) 
        ) , "La taille de l'image doit √™tre un tuple de deux entiers strictement positifs !"
        assert isinstance(test_size , float) and  1 > test_size > 0 , "la taille de test doit √™tre un r√©el entre 0 et 1 !"
        assert isinstance(val_size , float) and  1 > val_size > 0 , "la taille de validation doit √™tre un r√©el entre 0 et 1 !"
        assert isinstance(stratification , bool) , "L'argument 5 'stratification' doit prendre soit 'True' soit 'False' comme valeur!"
        assert isinstance(sameResultOfSplit , bool) , "L'argument 6 'sameResultOfSplit' doit prendre soit 'True' soit 'False' comme valeur!"

        #chargement du data selon le deux cas: √† partir d'un fichier csv ou √† partir d'un r√©pertoire
        if file_namesAndLabelsOfImages == None:
            data_dataFrame = self.loadDataFromDirectory(path)
        else:
            data_dataFrame = self.loadData_csvFile(path , file_namesAndLabelsOfImages)

        #encodage des labels
        data_dataFrame = self.labelsEncoder(data_dataFrame , 'label')
        #transformation des images en gris, les redimensionner et les transformer en matrices et la normalisation
        X , y = self.processAllImages(data_dataFrame , sizeOfImage)
        #division des donn√©es en train, validation et test
        X_temp , X_test , y_temp , y_test = self.splitData(X , y , test_size , True , True)
        X_train , X_val , y_train , y_val = self.splitData(X_temp , y_temp , val_size*1.25 , True , True)

        #Encodage One-Hot des targets
        y_train = self.oneHotEncoder(y_train)
        y_val = self.oneHotEncoder(y_val)
        y_test = self.oneHotEncoder(y_test)

        #validation des sorties
        assert X_train.shape[0] == y_train.shape[0] , "les premi√®res dimensions de X_train et y_train doit √™tre √©gales !"
        assert X_val.shape[0] == y_val.shape[0] , "les premi√®res dimensions de X_val et y_val doit √™tre √©gales !"
        assert X_test.shape[0] == y_test.shape[0] , "les premi√®res dimensions de X_test et y_test doivent √™tre √©gales !"
        assert X_train.shape[1] == X_val.shape[1] == X_test.shape[1] , "X_train, X_val, X_test doivent avoir le m√™me nombre de features !"
        assert y_train.shape[1] == y_val.shape[1] == y_test.shape[1] , "y_train, y_val, y_test doivent avoir la m√™me deuxi√®me dimension !"

        return X_train , X_val , X_test , y_train , y_val , y_test



########################################### Classe prePorcessingDataForCNN  #################################################################################

class ImagesDataPreprocessingForCNN:
    """
    Cette Classe fournit des donn√©es images pr√™tes √† utiliser dans un mod√®le CNN.
    Elle rassemble des m√©thodes de pr√©traitement des donn√©es images et le sur-√©chantillonage. ces m√©thodes sont des m√©thodes de la classe donc 
    elles vont √™tre appel√©es sans l'instanciation de la classe.
    la m√©thode fit_transforme ex√©cute le pipeline de pr√©taitement et fournit comme sortie les ensembles de train, validation et de test avec les target
    cod√©s en One-hot pour √™tre compatible avec la fonction d'activation softmax et la fonction de perte 'cross-entropy'
    """
    def __init__(self):
        pass
        
    @classmethod
    def loadData_csvFile(self , path: str , file_namesAndLabelsOfImages: str) -> pd.DataFrame:
        """
         le chargement du fichier contenant les chemins vers les images et les labels en les mettant dans un DataFrame qui sera l'objet de return
         
        """
        #validation de l'entr√©e
        assert isinstance(path , str) , "L'argument 1 pass√©e √† cette m√©thode doit √™tre un chemin vers un dossier 'string' !"
        assert isinstance(file_namesAndLabelsOfImages , str) , "L'arguement 2 doit √™tre un nom d'un fichier csv 'String' !"

        #d√©finition du chemin du dossier contenant les images et le fichier csv (noms des images + labels)
        pathOfDataDirectory = os.path.join(os.getcwd() , path)

        #chargement du fichier csv contenant √† la fois les noms des images et leurs √©tiquettes
        try:
            labelsAndPahs = pd.read_csv(os.path.join(pathOfDataDirectory , file_namesAndLabelsOfImages) , header= None)
            #ajouter les ent√™tes aux DataFrame
            labelsAndPahs.columns = ['image_path', 'label'] 
            
            """
             Supprimer le pr√©fixe "images-data-64/" du chemin si pr√©sent et cela concerne seulement notre tp3 car le fichier csv contient 
            un r√©pertoire qui ne figure pas dans le vrai chemin et en plus des slash √† la place des anti-slash
            """
            labelsAndPahs['image_path'] = labelsAndPahs['image_path'].apply(lambda p: p[len("./images-data-64/"):]
                                                                            if p.startswith("./images-data-64/") else p)

            #v√©rification de l'existance des noms des images et de leurs √©tiquettes dans le fichier csv
            assert (
                'image_path' in labelsAndPahs.columns and 'label' in labelsAndPahs.columns 
            ), "Le fichier doit contenir les noms des fichiers images et le √©tiquettes de chacune d'elles !"

            #ajout du r√©pertoire racine aux chemins de toutes les images
            labelsAndPahs['image_path'] = labelsAndPahs['image_path'].apply(lambda x: 
                                                                            os.path.normpath(os.path.join(pathOfDataDirectory, x.lstrip("/\\")))) 
            
            print(f"T√©l√©chargement de {len(labelsAndPahs)} √©chantillons avec {labelsAndPahs['label'].nunique()} classes")
            
            return labelsAndPahs
            
        except FileNotFoundError:
            print(f"Le fichier {file_namesAndLabelsOfImages} est introuvable !")
            return pd.DataFrame()

    @classmethod
    def loadDataFromDirectory(self , path: str) -> pd.DataFrame:
        # Consruction du DataFrame √† partir du r√©pertoire contenant des images
        #validation de l'entr√©e
        assert isinstance(path , str) , "L'argument doit √™tre le chemin vers le r√©pertoire des images sous forme de string !"
        imagePaths = []   #liste des chemins complets des images
        labels = []       #liste des √©tiquettes des images
        #d√©finition du chemin du dossier contenant les images et le fichier csv (noms des images + labels)
        pathOfDataDirectory = os.path.join(os.getcwd() , path)
        
        for subDirName in os.listdir(pathOfDataDirectory):
            subDirPath = os.path.join(pathOfDataDirectory , subDirName)
            if os.path.isdir(subDirPath):
                for imageName in os.listdir(subDirPath):
                    imagePaths.append(os.path.join(subDirPath , imageName))
                    labels.append(subDirPath)
        
        # Cr√©ation du DataFrame
        data_labels_imagesPaths = pd.DataFrame({'image_path': imagePaths , 'label': labels})
        
        # V√©rification du DataFrame avant de le renvoyer
        assert not data_labels_imagesPaths.empty, "Le data n'est pas t√©l√©charg√©, v√©rifier les chemins et le dossier source !"
        
        print(f"T√©l√©chargement de {len(data_labels_imagesPaths)} √©chantillons avec {data_labels_imagesPaths['label'].nunique()} classes")

        return data_labels_imagesPaths

    @classmethod
    def labelsEncoder(self , data: pd.DataFrame , labels: str ) -> pd.DataFrame:
        #Encodage  des labels des images
        #validation des entr√©es
        assert isinstance(data , pd.DataFrame) , "L'argument 1 doit √™tre une DataFrame !"
        assert isinstance(labels , str) and labels in data.columns , "L'argument 2 doit √™tre un nom d'une colonne de DataFrame 'arg1' !"
        
        encoder = LabelEncoder()
        data[labels] = encoder.fit_transform (data[labels])
        
        num_classes = len(encoder.classes_)
        #validation de l'op√©ration de l'encodage
        assert data[labels].nunique() == num_classes , "L'op√©ration de l'encodage n'est pas bien aboutie !"

        return data

    @classmethod
    def loadAndProcessImage(self , imagePath: str , sizeOfImage: tuple[int , int] , mode: str= 'RGB') -> np.ndarray:
        """
            charge l'image et la redimensionner avant de la rendre sous forme d'un'un tableau ((n , n , 3) -> RGB ou (n , n , 1) -> niveau de gris)
            selon le choix pass√© par l'argument 'mode'.
        """
        #validation des entr√©es
        assert isinstance(imagePath , str) , "L'argument 1 est un chemin vers une image sous forme de 'String' !"
        assert os.path.exists (imagePath) , f"L'image est introuvable: {imagePath } !"
        assert (
            isinstance(sizeOfImage , tuple) and np.all(isinstance(number , int)  and  number > 0 for number in sizeOfImage) 
        ) , "La taille de l'image doit √™tre un tuple de deux entiers strictement positifs !"
        assert isinstance(mode , str) and mode in ['RGB' , 'Gray'] , "l'argument 3 'mode' doit √™tre 'RGB' ou 'Gray' !"

        if mode == 'Gray':
            #mettre l'image en gris 
            image = cv2.imread(imagePath , cv2.IMREAD_GRAYSCALE)
            
        else:
            #l'image en RGB
            image = cv2.imread(imagePath, cv2.IMREAD_COLOR)
            
        assert image is not None , f"L'image est introuvable: {imagePath } !"
        #redimensionnement de l'image
        image = cv2.resize(image , sizeOfImage)
        
        #normaliser l'image (x - min)/(max-min)
        image = image.astype(np.float32 )/255.0 

        #validation de la sortie
        assert isinstance(image , np.ndarray) and all(np.all((card >= 0) & (card <= 1)) for card in image) , \
                                                                                                    "L'image n'√©tait pas tra√Æt√©e correctement !"
        if mode == 'Gray':
            assert image.shape == (sizeOfImage) , "Le redimensionnement de l'image n'√©tait pas fait correctement !"
        else:
            assert image.shape == sizeOfImage + (3,) , "Le redimensionnement de l'image n'√©tait pas fait correctement !"
        
        return image

    @classmethod
    def processAllImages(self , labelsAndPaths: pd.DataFrame , sizeOfImage: tuple[int , int] , mode: str= 'RGB') -> np.ndarray:
        """
            transforme toutes les images en tableau multidimensionnel  et la target en vecteur
        """
        #validation des entr√©es
        assert isinstance(labelsAndPaths , pd.DataFrame) , "L'argument 1 doit √™tre de type 'DataFrame' de pandas !"
        assert (
                'image_path' in labelsAndPaths.columns and 'label' in labelsAndPaths.columns 
                ), "Le Dataframe 'argument 1' doit contenir les chemins vers les fichiers images et le √©tiquettes de chacune d'elles !"
        assert (
                  isinstance(sizeOfImage , tuple) and np.all(isinstance(number , int)  and  number > 0 for number in sizeOfImage) 
                ) , "La taille de l'image doit √™tre un tuple de deux entiers strictement positifs !"
        assert isinstance(mode , str) and mode in ['RGB' , 'Gray'] , "l'argument 3 'mode' doit √™tre 'RGB' ou 'Gray' !"
        
        #pr√©taitement des images
        X = np.array([self.loadAndProcessImage(path , sizeOfImage , mode) for path in tqdm(labelsAndPaths['image_path'])])
        y = self.labelsEncoder(labelsAndPaths , 'label' )['label'].values

        # v√©rification des  dimensions
        assert X.shape[0] == y.shape[0] , "Dimension incoh√©rent entre les donn√©es X et target y !"
        if mode == 'Gray':
            d = (len(labelsAndPaths['image_path']),) + sizeOfImage  
            assert (    
                X.shape == d
            ), f"le shape de X  {X.shape} doit correspondre √† {d} !"
        else:
            d = (len(labelsAndPaths['image_path']),) + sizeOfImage + (3,)
            assert (
                      X.shape == d
            ), f"le shape de X  {X.shape} doit correspondre √† {d} !"
            

        return X , y


    @classmethod
    def splitData(self , X: np.ndarray , y: np.ndarray , test_size: float , stratification: bool= True , 
                  sameResultOfSplit: bool= True)  -> tuple[np.ndarray , ...]:
        #divise les donn√©es et le traget en sous ensembles de train, de validation et de test
        #validation des entr√©es
        assert isinstance(X , np.ndarray) , "l'argument1 doit √™tre de type ndarray de numpy !"
        assert isinstance(y , np.ndarray) , "l'argument 2 doit √™tre de type ndarray de numpy !"
        assert X.shape[0] == y.shape[0] , "des donn√©es et le target doivent avoir le m√™me nombre d'enregistrements !"
        assert isinstance(test_size , float) and  1 > test_size > 0 , "la taille de test doit √™tre un r√©el entre 0 et 1 !"
        assert isinstance(stratification , bool) , "L'argument 3 'stratification' doit prendre soit 'True' soit 'False' comme valeur!"
        assert isinstance(sameResultOfSplit , bool) , "L'argument 4 'sameResultOfSplit' doit prendre soit 'True' soit 'False' comme valeur!"
        

        #division en train et test selon les param√®tres 'booleen' pass√©s √† la m√©thode
        if stratification:
            if sameResultOfSplit:
                X_train , X_test , y_train , y_test = train_test_split(X , y , test_size= test_size  , stratify= y , random_state= 42)
            else:
                X_train , X_test , y_train , y_test = train_test_split(X , y , test_size= test_size , stratify= y)
        else:
            if sameResultOfSplit:
                X_train , X_test , y_train , y_test = train_test_split(X , y , test_size= test_size , random_state= 42)
            else:
                X_train , X_test , y_train , y_test = train_test_split(X , y , test_size= test_size)

        #validation des r√©sultat avant de les retourner
        assert X_train.shape[0] == y_train.shape[0] , "X_train et y_train doivent avoir le m√™me nombre d'√©chantillons!"
        assert X_train.shape[0] == y_train.shape[0] , "X_test et y_test doivent avoir le m√™me nombre d'√©chantillons!"
        assert X_train.shape[1] == X_test.shape[1] , "X_train et X_test doivent avoir le m√™me nombre de variables!"

        return X_train , X_test , y_train , y_test

    @classmethod
    def oneHotEncoder(self , y: np.ndarray) -> np.ndarray:
        #encode les labels en one-hot: sous forme de veteur contenant des z√©ros et un
        #validation de l'entr√©e
        assert isinstance(y , np.ndarray) and y.ndim == 1 , "L'argument doit √™tre de type ndarray d'une seule dimension !"

        y_one_hot = np.array(OneHotEncoder(sparse_output=False).fit_transform(y.reshape(-1, 1)))

        #validation de la sortie
        assert (
            isinstance(y_one_hot , np.ndarray) and y_one_hot.shape == (y.shape[0] , len(np.unique(y)))
        ) , "Le r√©sultat de oneHotEncoder n'est un ndarray ou ses dimensions sont incorrectes !"

        return y_one_hot

    @classmethod
    def fit_transform(self , path: str , file_namesAndLabelsOfImages: str= None , sizeOfImage: tuple[int , int]= (32 , 32) , mode: str= 'RGB',
    val_size: float= 0.2 , test_size: float= 0.2 ,  stratification: bool= True , sameResultOfSplit: bool= True) -> tuple[np.ndarray, ...]:
        """        
            cette m√©thode est un pipeline qui fait appel √† les autres m√©thode pour charger, pr√©traiter et diviser le data et retourne en fin 
            retourner les ensembles de train, de validation et de test.
        """
        #validation des l'entr√©es
        assert isinstance(path , str) , "L'argument 1 pass√©e √† cette m√©thode doit √™tre un chemin vers un dossier 'string' !"
        assert isinstance(file_namesAndLabelsOfImages , (type(None) , str)) , "L'arguement 2 doit √™tre un nom d'un fichier csv 'String' ou None !"
        assert (
            isinstance(sizeOfImage , tuple) and np.all(isinstance(number , int)  and  number > 0 for number in sizeOfImage) 
        ) , "La taille de l'image doit √™tre un tuple de deux entiers strictement positifs !"
        assert isinstance(mode , str) and mode in ['RGB' , 'Gray'] , "l'argument 4 'mode' doit √™tre 'RGB' ou 'Gray' !"
        assert isinstance(test_size , float) and  1 > test_size > 0 , "la taille de test doit √™tre un r√©el entre 0 et 1 !"
        assert isinstance(val_size , float) and  1 > val_size > 0 , "la taille de validation doit √™tre un r√©el entre 0 et 1 !"
        assert isinstance(stratification , bool) , "L'argument 7 'stratification' doit prendre soit 'True' soit 'False' comme valeur!"
        assert isinstance(sameResultOfSplit , bool) , "L'argument 8 'sameResultOfSplit' doit prendre soit 'True' soit 'False' comme valeur!"

        #chargement du data selon le deux cas: √† partir d'un fichier csv ou √† partir d'un r√©pertoire
        if file_namesAndLabelsOfImages == None:
            data_dataFrame = self.loadDataFromDirectory(path)
        else:
            data_dataFrame = self.loadData_csvFile(path , file_namesAndLabelsOfImages)

        print(data_dataFrame.columns)
        #encodage des labels
        data_dataFrame = self.labelsEncoder(data_dataFrame , 'label')
        #transformation des images en gris ou en RGB, les redimensionner et les transformer en matrices et la normalisation
        X , y = self.processAllImages(data_dataFrame , sizeOfImage , mode)
        #division des donn√©es en train, validation et test
        X_temp , X_test , y_temp , y_test = self.splitData(X , y , test_size , True , True)
        X_train , X_val , y_train , y_val = self.splitData(X_temp , y_temp , val_size*1.25 , True , True)

        #Encodage One-Hot des targets
        y_train = self.oneHotEncoder(y_train)
        y_val = self.oneHotEncoder(y_val)
        y_test = self.oneHotEncoder(y_test)

        #validation des sorties
        assert X_train.shape[0] == y_train.shape[0] , "les premi√®res dimensions de X_train et y_train doit √™tre √©gales !"
        assert X_val.shape[0] == y_val.shape[0] , "les premi√®res dimensions de X_val et y_val doit √™tre √©gales !"
        assert X_test.shape[0] == y_test.shape[0] , "les premi√®res dimensions de X_test et y_test doivent √™tre √©gales !"
        assert X_train.shape[1] == X_val.shape[1] == X_test.shape[1] , "X_train, X_val, X_test doivent avoir le m√™me nombre de features !"
        assert y_train.shape[1] == y_val.shape[1] == y_test.shape[1] , "y_train, y_val, y_test doivent avoir la m√™me deuxi√®me dimension !"

        return X_train , X_val , X_test , y_train , y_val , y_test



######################################################### classe PreprocessingData  ######################################################################

class PreprocessingData:
    """
     La classe 'PreprocessingData' pour le pr√©traitement des donn√©es et ces arguments d'instanciation sont:
        # numericalOutliersStrategy: strat√©gie de tra√Ætement des outliers ['keep': les garder , 'mean': les remplacer par Moyenne, 'median': par la m√©diane ]
        # normalisationStrategy: la technique pour la mise √† la m√™me √©chelle des variables num√©riques ['normalisation': [0,1] ,'standardisation':
        [m=0 et sigma = 1 ] ]
        # missingNumericalValuesStrategy: m√©thode pour tra√Æter les vals. num. manqantes ['drop': supprimer les lignes contenant des vals. manqs. , 
        'mean': remp. par  moy , 'median': remp. par mediane ]
        # missingCategoricalValuesStrategy: strat√©gie pour tra√Æter les vals. manq. cat√©g. ['drop': supprimer les ligne avec des vals. manq. , 'most': 
        remp. par mode , 'less': remp. par la modalit√© la moins fr√©quente.
        # ordinalcategoricalVariables: liste des noms des variables (ent√™tes d'un fichier csv par ex.) cat√©go. ordinales
        # numericalsWithInvalidZeros: liste des noms des var. num. dont la valeur 0 est jug√©e invalide ou insignifiante (ex. rythme cardiaque==0 ==> 
        mort pas logique!)
        # de plus, la fonction divise le dataset en train et test avec une m√©thode dont dispose appel√©e splitData(...) avec des arguments test_size, 
        stratification (True par d√©faut / False) et sameResultOfSplit (True par d√©faut / False)
        
    """
    
    def __init__(self , numericalOutliersStrategy: str= 'keep' , normalisationStrategy: str='standardisation' , 
                 missingNumericalValuesStrategy: str= 'mean' , missingCategoricalValuesStrategy: str= 'most' , 
                 ordinalcategoricalVariables: list[str]= None , numericalsWithInvalidZeros: list[str]= None):
        #verification des entr√©es
        assert numericalOutliersStrategy in ['mean' , 'median' , 'keep'] , "la technique de tra√Æter les outliers doit √™tre 'mean' ou 'median' ou 'keep'"
        assert normalisationStrategy in ['standardisation' , 'normalisation'] , (
            "la technique de normalisation doit √™tre 'standardisation'ou 'normalisation'"
        )
        
        assert missingNumericalValuesStrategy in ['drop' , 'mean' , 'median'] , (
            """les valeurs num√©riques manquantes doivent √™tre tra√Æt√©es comme suit: suppression de ligne: 'drop', remplacement par la moyenne:
            'mean'ou par la m√©diane 'median'"""
            )
       
        assert missingCategoricalValuesStrategy in ['drop' , 'most' , 'less'] , (
            """les valeurs cat√©gorielles manquantes doivent √™tre tra√Æt√©es comme suit: suppression de ligne: 'drop', remplacement par le mode:
            'most'ou par la modalit√© la moins fr√©quentes 'less'""")
        
        assert ordinalcategoricalVariables is None or (isinstance(ordinalcategoricalVariables, list) and
                                                       all(isinstance(var, str) for var in ordinalcategoricalVariables)), (
        "L'argument 'ordinalcategoricalVariables' doit √™tre une liste de cha√Ænes (noms de variables ordinales) ou None."
        )

        assert numericalsWithInvalidZeros is None or (isinstance(numericalsWithInvalidZeros, list) and
                                                       all(isinstance(var, str) for var in numericalsWithInvalidZeros)), (
        "L'argument 'numericalsWithInvalidZeros' doit √™tre une liste de: (noms de variables ordinales dont la valeur 0 est insignifiante) ou (None)!"
        )

        #initialisation des attributs
        self.numericalOutliersStrategy = numericalOutliersStrategy
        self.normalisationStrategy = normalisationStrategy
        self.missingNumericalValuesStrategy = missingNumericalValuesStrategy
        self.missingCategoricalValuesStrategy = missingCategoricalValuesStrategy
        self.data = None
        self.preProcessedData = None
        self.categoricalVariables = None
        self.numericalVariables = None
        self.ordinalcategoricalVariables = ordinalcategoricalVariables
        self.numericalsWithInvalidZeros = numericalsWithInvalidZeros

    def separateTypeOfVaraibles(self):
        #s√©paration des deux type de variables contenues dans le dataset pour les pr√©parer aux tra√Ætement
        self.numericalVariables = self.data.select_dtypes(include = ['number'])
        self.categoricalVariables = self.data.select_dtypes(exclude = ['number'])
        
    
    def missingNumericalValuesProcess(self):
        ## tra√Ætement des valeurs num√©riques manquantes selon la strat√©gie choisie (moyenne , m√©diane , suppression) pass√©e 
        ## lors de l'inctanciation de l'objet PreprocessingData
        #v√©rification de l'attribut 'numericalVariables'
        assert isinstance(self.numericalVariables , pd.DataFrame) , "L'attribut 'numericalVariables' doit √™tre un DataFrame pandas !"
        assert self.numericalVariables is not None and not self.numericalVariables.empty, "'numericalVariables' ne doit pas √™tre None ni vide !"

        #traitement des valeurs manquantes selon la strat√©gie choisie
        if self.missingNumericalValuesStrategy == 'mean':
            self.numericalVariables.fillna(self.numericalVariables.mean() , inplace= True)
            
        elif self.missingNumericalValuesStrategy == 'median':
            self.numericalVariables.fillna(self.numericalVariables.median() , inplace= True)
        else:
            self.numericalVariables.dropna(inplace= True)

   
    def missingCategoricalValuesProcess(self):
        # # tra√Ætement des valeurs cat√©gorielles manquantes selon la strat√©gie choisie (mode , modalit√© moins fr√©quente , suppression)
        #v√©rification de l'attribut 'categoricalVariables'
        assert isinstance(self.categoricalVariables , pd.DataFrame) , "L'attribut 'categoricalVariables' doit √™tre un DataFrame pandas !"
        assert self.categoricalVariables is not None and not self.categoricalVariables.empty, "'categoricalVariables' ne doit pas √™tre None ni vide !"

        #tra√Ætement des valeurs manquantes selon la strat√©gie choisie
        if self.missingCategoricalValuesStrategy == 'most':
            for var in tqdm(self.categoricalVariables.columns , desc= "Tra√Ætement des valeurs cat√©gorielles manquantes:"):
                mostModality = self.categoricalVariables[var].mode()[0]
                self.categoricalVariables.fillna(mostModality , inplace= True)
            
        elif self.missingCategoricalValuesStrategy == 'less':
            for var in tqdm(self.categoricalVariables.columns , desc= "Tra√Ætement des valeurs cat√©gorielles manquantes:"):
                lessModality = self.categoricalVariables[var].value_counts().idxmin()
                self.categoricalVariables.fillna(lessModality , inplace= True)
        else:
            self.categoricalVariables.dropna(inplace= True)

    def normalizeOrStandardizeNumericalVariables(self , target: str= None):
        #la normalisation ou la standardisation des variables num√©riques selon le choix fait lors de l'instanciation de l'objet de la pr√©sente classe
        #argument 'target' sert √† l'excepter de la normalisation ou la standardisation s'il figure parmi les variables num√©riques
        #v√©rifications
        assert isinstance(self.numericalVariables , pd.DataFrame) , "L'attribut 'numericalVariables' doit √™tre un DataFrame pandas !"
        assert self.numericalVariables is not None and not self.numericalVariables.empty, "'numericalVariables' ne doit pas √™tre None ni vide !"
        assert not self.numericalVariables.isnull().values.any() , ( 
        "l'attribut 'numericalVariables' contient des valeurs manquantes! Il faut les tra√Æter d'abord."
        )
        assert isinstance(target , str) and target in self.data.columns , "l'argument 2 'target' doit √™tre un nom (string) d'une variable de dataset!"

        #affichage de la barre de progression de l'op√©ration de normalisation ou de standardisation evec la biblioth√®que python 'tqdm'
        with tqdm(total=1 , desc=f"{self.normalisationStrategy.capitalize()} des variables num√©riques" , unit= "√©tape") as pbar:
        
            #instanciation de normaliseur et de standardiseur
            standardScaler = StandardScaler()
            minMaxScaler = MinMaxScaler()
    
            #isoler le target pour l'excepter de la normal. /stand. s'il est renseign√© lors de la pelle √† la fm√©thode fit_transform
            if target in self.numericalVariables.columns:
                target_var = self.numericalVariables[[target]].copy()
                self.numericalVariables.drop(target , axis= 1 , inplace= True)
                
            #la standardisation ou la normalisation des variables
            if self.normalisationStrategy == 'standardisation':
                self.numericalVariables = pd.DataFrame(standardScaler.fit_transform(self.numericalVariables) , columns= self.numericalVariables.columns)
            else:
                self.numericalVariables = pd.DataFrame(minMaxScaler.fit_transform(self.numericalVariables) , columns= self.numericalVariables.columns)
    
            #rassembler les variables num√©riques avec le target si il √©tait isol√© ci-dessus 
            if target != None:
                self.numericalVariables = pd.concat([self.numericalVariables , target_var] , axis= 1)

            pbar.update(1)
            
    def encodeCategoricalVariables(self):
        #encoder les variables cat√©gorielles selon le type ordinal ou non ordinal
        #v√©rifications
        assert isinstance(self.categoricalVariables , pd.DataFrame) , "L'attribut 'categoricalVariables' doit √™tre un DataFrame pandas !"
        assert self.categoricalVariables is not None and not self.categoricalVariables.empty, "'numericalVariables' ne doit pas √™tre None ni vide !"
        assert not self.categoricalVariables.isnull().values.any() , (
        "l'attribut 'categoricalVariables' contient des valeurs manquantes! Il faut les tra√Æter d'abord!"
        )
        #instanciation des encodeurs ordinal et non ordinal
        encoderNotOrdinal = OneHotEncoder( sparse_output = False)
        encoderOrdinal = OrdinalEncoder()
        #Encoder les variables cat√©gorielles
        for var in tqdm(self.categoricalVariables.columns , desc= "Encodage des variables cat√©gorielles:"):
            if self.ordinalcategoricalVariables is not None and var in self.ordinalcategoricalVariables:
                self.categoricalVariables[var] = encoderOrdinal.fit_transform(self.categoricalVariables[[var]])
            else:
                self.categoricalVariables[var] = encoderNotOrdinal.fit_transform(self.categoricalVariables[[var]])

    def manageOutliersInNumericalVariables(self):
        #traiter les valeurs ab√©rrantes par la strat√©gie choisie('mean': remplacemnt par la moyenne , 'median' : par la m√©diane
        #, 'keep': garder les) pendant l'instanciation de l'objet PreprocessingData
        #v√©rifications
        assert isinstance(self.numericalVariables , pd.DataFrame) , "L'attribut 'numericalVariables' doit √™tre un DataFrame pandas !"
        assert self.numericalVariables is not None and not self.numericalVariables.empty, "'numericalVariables' ne doit pas √™tre None ni vide !"
        assert not self.numericalVariables.isnull().values.any() , (
        "l'attribut 'numericalVariables' contient des valeurs manquantes! Il faut les tra√Æter d'abord."
        )
        for var in tqdm(self.numericalVariables.columns , desc= "Tra√Ætement des valeurs num√©riques aberrantes:"):
            if self.numericalOutliersStrategy == 'keep':
                break
            else: 
                #d√©tection des outliers par la m√©thode IQR
                Q1 = self.numericalVariables[var].quantile(0.25) # 1er quartile 
                Q3 = self.numericalVariables[var].quantile(0.75) # 3e quartile
                IQR = Q3 - Q1
                lowerBound = Q1 - 1.5 * IQR 
                upperBound = Q3 + 1.5 * IQR
                if self.numericalOutliersStrategy =='mean':
                    self.numericalVariables[var] = np.where((self.numericalVariables[var] < upperBound) | 
                            (self.numericalVariables[var] > bord_superieur), self.numericalVariables[var].mean(), self.numericalVariables[var])
                else:
                    self.numericalVariables[var] = np.where((self.numericalVariables[var] < upperBound) | 
                            (self.numericalVariables[var] > bord_superieur), self.numericalVariables[var].median(), self.numericalVariables[var])
            
    def replaceInvalidZerosWithMedian(self):
        #remplace les z√©ros invalide c'est-√†-dire insignifiants par la m√©diane
        #v√©rification de l'attribut 
        assert isinstance(self.numericalVariables , pd.DataFrame) , "L'attribut 'numericalVariables' doit √™tre un DataFrame pandas !"
        assert (self.numericalVariables is not None) and (not self.numericalVariables.empty), "'numericalVariables' ne doit pas √™tre None ni vide !"
        assert not self.numericalVariables.isnull().values.any() , (
        "l'attribut 'numericalVariables' contient des valeurs manquantes! Il faut les tra√Æter d'abord."
        )
        #remplacement des z√©ros invalides par la m√©diane
        if self.numericalsWithInvalidZeros != None and len(self.numericalsWithInvalidZeros) > 0:
            for var in tqdm(self.numericalsWithInvalidZeros , desc= "Remplacment des z√©ros invalides:"):
                median = self.numericalVariables.loc[self.numericalVariables[var] != 0 ,var].median()
                self.numericalVariables[var] = self.numericalVariables[var].replace(0, median)


    def fit_transform(self , X: pd.DataFrame , target: str= None) -> pd.DataFrame:
        #le pr√©traiment de dataset d'entr√©e selon les param√®tres transmis au constructeur de la classe 'PreprocessingData'
        #le r√©sultat sera affect√© √† l'attribut preprocessedData renvoy√© apr√®s sous forme d'un DataFrame
        
        #v√©rivication de la premi√®re'entr√©e
        assert isinstance(X , pd.DataFrame) , "l'argument pass√© √† cette m√©thode est de type DataFrame de Pandas"
        assert isinstance(target , str) and target in X.columns , "l'argument 2 'target' doit √™tre un nom (string) d'une variable de dataset!"
        self.data = X.copy()
        #pr√©traitement du dataset
        self.separateTypeOfVaraibles()            #s√©paration des variables
    
        if self.numericalVariables is not None and not self.numericalVariables.empty:
            self.missingNumericalValuesProcess()      #tra√Ætement des valeurs num√©riques manquantes
            print('Tra√Ætement des valeurs num√©riques manquantes -->','Strat√©gie:' ,self.missingNumericalValuesStrategy)
            self.replaceInvalidZerosWithMedian()  #remplacement des z√©ros jug√©s invalides dans les variables num√©riques (ex: 0 en √¢ge est invalide)
            print('Remplacement des 0 invalides -->','dans les variables:', self.numericalsWithInvalidZeros , 'Stat√©gie: median')
            self.manageOutliersInNumericalVariables()      #tra√Ætement des valeurs num√©riques ab√©rrantes
            print('Tra√Ætement des valeurs num√©riques aberrantes -->', 'Strat√©gie', self.numericalOutliersStrategy)
            self.normalizeOrStandardizeNumericalVariables(target)    #normalisation ou standardisation des variables num√©riques
            print('Les donn√©es sont mises √† la m√™me √©chelle -->', 'Strat√©gie:' , self.normalisationStrategy)

        if self.categoricalVariables is not None and not self.categoricalVariables.empty:
            self.missingCategoricalValuesProcess()    #tra√Ætement des valeurs cat√©gorielles manquantes
            print('Tra√Ætement des valeurs cat√©gorielles manquantes -->','Strat√©gie:' ,self.missingCategoricalValuesStrategy)
            self.encodeCategoricalVariables()                # encodage des variables cat√©gorielles
            print('Encodage des variables cat√©gorielles non ordinales-->','Strat√©gie: OneHoteEncoder')
            print('Encodage des variables cat√©gorielles  ordinales-->','variables:',self.ordinalcategoricalVariables , 'Strat√©gie: OrdinalEncoder')
        


        #affectation du r√©sultat √† l'attribut preProcessedData
        print("üéâ Le dataset est pr√©tra√Æt√© selon les choix ci-dessus que vous avez faits lors de la cr√©ation de l'objet de cette classe üéâ")
        
        self.preProcessedData = pd.concat( [self.numericalVariables , self.categoricalVariables] , axis = 1 )

        #v√©rification de l'attribut preProcessedData avant de le renvoyer
        assert (self.numericalVariables is not None) and (not self.numericalVariables.empty) , (
                                                "l'op√©ration de pr√©processing n'est pas aboutie, l'attribut 'preProcessedData' est vide!"
        )
        assert isinstance(self.preProcessedData , pd.DataFrame) , "la valeur de retour de la fonction 'fit_transform' est de type DataFrame!"

        return self.preProcessedData

    def splitData(self , target: str= None , test_size: float= 0.2 , stratification: bool= True ,
                                                               sameResultOfSplit: bool= True) -> tuple[np.ndarray , ...]:
        #s√©paration de la target des features et la division des deux en deux ensembles d'entra√Ænement et de test:
        #X_train , y_train , X_test et y_test qui seront renvoy√©s en fin de la fonction
        #validation des entr√©es
        assert self.preProcessedData is not None , "Le data n'est pas encore tra√Æter, preProcessedData est None!"
        assert isinstance(target , str) and target in self.preProcessedData.columns , "L'argument 1 'target' doit √™tre une colonne de dataframe 'arg1'!"
        assert isinstance(test_size , float) and  0 < test_size < 1 , "La taille de test 'test_size' doit √™tre un r√©el entre 0 et 1 !"
        assert isinstance(stratification , bool) , "L'argument 3 'stratification' doit prendre soit 'True' soit 'False' comme valeur!"
        assert isinstance(sameResultOfSplit , bool) , "L'argument 4 'sameResultOfSplit' doit prendre soit 'True' soit 'False' comme valeur!"

        #s√©paration du la target des autres features
        features = (self.preProcessedData.drop([target] , axis= 1)).to_numpy()
        target = (self.preProcessedData[[target]]).to_numpy()

        #division en train et test selon les param√®tres 'booleen' pass√©s √† la m√©thode
        if stratification:
            if sameResultOfSplit:
                X_train , X_test , y_train , y_test = train_test_split(features , target , test_size= test_size  , stratify= target , random_state= 42)
            else:
                X_train , X_test , y_train , y_test = train_test_split(features , target , test_size= test_size , stratify= target)
        else:
            if sameResultOfSplit:
                X_train , X_test , y_train , y_test = train_test_split(features , target , test_size= test_size , random_state= 42)
            else:
                X_train , X_test , y_train , y_test = train_test_split(features , target , test_size= test_size)

        #validation des r√©sultat avant de les retourner

        assert X_train.shape[0] == y_train.shape[0] , "X_train et y_train doivent avoir le m√™me nombre d'√©chantillons!"
        assert X_train.shape[0] == y_train.shape[0] , "X_test et y_test doivent avoir le m√™me nombre d'√©chantillons!"
        assert X_train.shape[1] == X_test.shape[1] , "X_train et X_test doivent avoir le m√™me nombre de variables!"

        return X_train , X_test , y_train , y_test
        
        